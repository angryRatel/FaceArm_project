# Face-Arm

# AI(Computer Vision)기반 협동 로봇 작업 어시스턴트 구현 프로젝트


# 주제 : Auto Weld(용접협동로봇)
두산로봇팔 모델 사용<br> 
-ROBOT_ID = "dsr01" <br>
-ROBOT_MODEL = "m0609"

## 실제 시연 영상 : https://youtu.be/wykA4MYREYk

### 코드 사용시 :  https://github.com/ROKEY-SPARK/DoosanBootcamp.git 받아서 파일구조 수정후 사용할것 

#### dsr_rokey/rokey/rokey/basic 안의 파일내용

## 작성하여 구현한 코드들

• object_detection <br>
• face_yolo
• robot_control <br>
• get_keyword <br>

## 간단한 설명



### object_detection <br>


![1](https://github.com/user-attachments/assets/42ba181a-3b21-40b5-bfd5-0deea709e732) <br>
학습 모델을 활용하여 물체를 인식



### face_yolo <br>

![2](https://github.com/user-attachments/assets/347fb22f-98f3-4879-83e5-42f350b7aa7a) <br>

학습모델을 활용하여 얼굴을 인식
카메라 화면의 x,y 좌표를 world 좌표계의 x,z로 리매핑 하여 얼굴의 위치 파악을 하게 해줌


### get_keyword <br>
• OOO 갖다 줘 <br>
• 돌아가 <br>
• 내려놔, 갖다놔 <br>
• 여기로 와 <br>
• 위/아래/오른쪽/왼쪽/앞/뒤 <br>
위와 같은 명령어 추출


### robot_control <br>
• OOO 갖다 줘 <br>
• 돌아가 <br>
• 내려놔, 갖다놔 <br>
• 여기로 와 <br>
• 위/아래/오른쪽/왼쪽/앞/뒤 <br>
특정 get_keyword.py 에서 추출된 명령어 이행



# 시나리오 정리

갖다 줘" 명령은 **사람에게 물건을 전달**하는 대표 기능입니다.

1. **Realsense D435 카메라**가 작업 공간을 스캔하여 물건의 좌표를 추출합니다.
    
    → 이는 `object_detection_node`에서 YOLO와 depth 데이터를 함께 활용합니다.
    
2. 추출한 좌표로 로봇이 이동하고, **그리퍼가 물체를 집습니다.**
    
    → 이때, 잡은 물체의 좌표는 리스트에 저장됩니다. (`picked_object_position` 리스트 등)
    
3. 이어서 **Logitech C270 카메라** 또는 Face YOLO를 통해 **사람의 얼굴 좌표를 감지**합니다.
4. 로봇이 해당 좌표로 이동하여 **사람 앞에 정지**합니다.
5. 안내 멘트를 통해 “물건을 가져가세요” 라고 전달한 후, **5초간 외력을 감지**합니다.
    - 사람이 물건을 가져가면 → 그리퍼를 열고 대기
    - 가져가지 않으면 → 물건을 계속 잡은 채로 다음 명령어 대기

> 이 로직은 force_monitor_node 또는 compliance_gripper 로직과 연동되어, 외력 판단 후 다음 행동을 결정합니다.
> 

---

### 🔁 “돌아가” 명령어

"돌아가"는 **로봇팔이 시야를 가릴 경우**, 다시 기본 위치로 복귀시키는 명령어입니다.

- 로봇은 미리 설정된 **홈 포지션 조인트 값**으로 `movej()` 명령을 실행하며 복귀합니다.
- 이후, 다시 Wake-up 명령을 대기합니다.

> 이는 robot_controller.py의 go_home() 함수로 구현됩니다.
> 

---

### 📦 “갖다 놔” 또는 “내려놔” 명령어

이 명령은 **물건을 사용자가 로봇에게 주었을 때**, 다시 원래 위치로 반납하는 기능입니다.

1. 사용자가 그리퍼 사이에 물건을 넣고 “갖다놔”라고 말하면,
    - 그리퍼가 **닫혀서 물건을 집습니다.**
2. 이전 "갖다 줘" 명령 시 **저장된 좌표로 이동**합니다.
3. **그리퍼를 열어 물체를 놓고**, 홈 위치로 복귀합니다.

> 중요한 부분은 그리퍼 상태를 판단해 이미 물건을 갖고 있는 경우 이 절차를 생략한다는 점입니다.
> 

---

### 🧍 “여기로 와” 명령어

사람이 **지정된 위치에서 부르면**, 로봇이 사람 쪽으로 이동합니다.

- Face YOLO로 사람의 좌표를 파악하고, 해당 좌표로 로봇이 이동합니다.
- 물체를 잡고 있는 경우에는 **역시 외력 감지**를 5초간 수행하여 반응합니다.

---

### ↔️ “오른쪽 / 왼쪽 / 위 / 아래 / 앞 / 뒤” 명령어

이 명령들은 **미세 위치 조정**을 위한 보조 명령입니다.

- 명령에 따라 로봇은 지정된 축 방향으로 **정해진 거리만큼 이동**합니다. (`offset_xyz(pose, dx, dy, dz)` 방식 사용)
- 그리퍼가 물체를 잡고 있는 경우 → 외력 감지 루틴이 함께 수행됩니다.

---

### 💡 기술적 포인트 요약

- **YOLO + Depth → 좌표 추출**
- **Gripper 제어 → Python + WebLogic 조건 기반**
- **Force Sensor → 외력 감지 후 논리 분기 처리**
- **Voice Command 인식 → LangChain 기반 키워드 추출**
- **로봇 동작 → ROS2 + Python 통신 구조로 모듈화**


# 프로젝트 내용 정리


### 1. 우리 프로젝트의 **특징 및 장점**

- **높은 호환성과 구조화된 코드 구성**
    
    클라이언트-서버 구조와 퍼블리셔-서브스크라이버를 활용해 기능별로 코드를 세분화하였으며,
    
    이를 통해 **유지보수가 용이**한 구조를 설계하였습니다.
    
- **직접 데이터 확보 및 라벨링을 통한 정확도 향상**
    
    오픈소스 및 공공 데이터만으로는 객체 인식 정확도가 부족하다고 판단하였고,
    
    전담 인원을 지정해 직접 데이터를 수집하고 라벨링을 진행함으로써
    
    **높은 정확도의 학습 모델을 구현**할 수 있었습니다.
    
- **예외처리 코드 분리 및 통합 적용 용이성 확보**
    
    실사용 환경에서 발생할 수 있는 다양한 예외 상황을 고려하여, 예외처리 기능들을 별도의 코드 및 모듈
    
    로 **독립 구성**하였습니다. 이를 통해 **기능 통합 시 재사용 용이성**과 안정성과 안전성을 확보하였습니다.
    

---

### 2. 프로젝트 구현 방향 및 방식

- **기능 우선순위에 따른 4단계 파트 구성**
    
    협동 1: 외력 제어(force control, …), 이동 명령
    
    협동 2: Modbus 통신(통신방식, 그리퍼 힘,너비 조절), 카메라 비전
    
    협동1, 2 에서 배운 기능들과 카메라 비전을 종합하여 **보조 협동 로봇 구현**에 필요한 핵심 기능들을
    
    **우선순위에 따라 총 4개의 파트로 나누고 역할을 분담**해 진행하였습니다.
    
    - **Part 1**: 안면 인식 및 객체 인식
    - **Part 2**: 키워드에 따라 도구 및 명령 수행 동작 기능 구현
    - **Part 3**: 음성 인식 문장에서 명령 키워드 추출
    - **Part 4**: 음성 인식 및 통합 적용 → 동작 기능 완성 후 통합만 진행 예정 (우선순위 낮음)

---

### 3. 프로젝트 중 추가 보완 사항

- **객체 인식 모델 개선**
    
    초기에는 오픈소스 및 공공데이터 기반으로 모델을 구축하였으나
    
    **정확도 부족 문제**로 인해 인원을 추가로 배분하여 직접 수집 및 라벨링 작업을 추가 진행하였고,
    
    그 결과 더 만족스러운 성능을 가진 객체 인식 모델을 완성할 수 있었습니다.


### 아티텍처 구조

![아티텍처](https://github.com/user-attachments/assets/318eaad1-0953-4cae-b1d4-166a47d41adf)


